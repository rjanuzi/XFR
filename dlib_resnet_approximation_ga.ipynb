{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition (FR) - DLIB ResNET Approximation with Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from time import time\n",
    "\n",
    "from random import random\n",
    "\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "from util._telegram import send_simple_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data and generate extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 5\n",
    "CLUSTER_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLIB_DISTANCES_FILE = Path(\"fr\", \"distances_dlib.json\")\n",
    "RESNET_DISTANCES_FILE = Path(\"fr\", \"distances_resnet.json\")\n",
    "RESNET_FACEPARTS_DISTANCES_FILE = Path(\"fr\", \"distances_resnet_faceparts.json\")\n",
    "DLIB_DATASET_CLUSTERS_FILE = Path(\"fr\", \"dlib_clusters.json\")\n",
    "\n",
    "BEST_INDIVIDUAL_FILE = Path(\"fr\", \"best_combination_runs\", f\"dlib_resnet_best_comb_{str(EXPERIMENT_ID).zfill(4)}.json\")\n",
    "BEST_INDIVIDUALS_FILE = Path(\"fr\", \"best_combination_runs\", f\"dlib_resnet_best_combs_{str(EXPERIMENT_ID).zfill(4)}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load distances from raw files into dataframes\n",
    "\n",
    "# DLIB Distances ( <pair>: {'dlib': distance}} )\n",
    "tmp_raw_data = json.load(open(DLIB_DISTANCES_FILE, \"r\"))\n",
    "dlib_distances = pd.DataFrame(dict(pair=tmp_raw_data.keys(), dlib_distance=(d['dlib'] for d in tmp_raw_data.values())))\n",
    "del tmp_raw_data\n",
    "\n",
    "# ResNET Faceparts Distances\n",
    "def rows_generator(resnet_faceparts_raw_data):\n",
    "    for pair, distances in resnet_faceparts_raw_data.items():\n",
    "        distances.update({'pair': pair})\n",
    "        yield distances\n",
    "\n",
    "tmp_raw_data = json.load(open(RESNET_FACEPARTS_DISTANCES_FILE, \"r\"))\n",
    "generator = rows_generator(tmp_raw_data)\n",
    "del tmp_raw_data\n",
    "\n",
    "resnet_faceparts_distances = pd.DataFrame(generator)\n",
    "\n",
    "# Join distances into a sigle dataframe\n",
    "distances = dlib_distances.merge(resnet_faceparts_distances, on='pair', how='outer')\n",
    "\n",
    "del dlib_distances\n",
    "del resnet_faceparts_distances\n",
    "\n",
    "# Filter only images with \"n\" (from VGGFACE2)\n",
    "distances = distances[distances.pair.apply(lambda p: \"n\" in p)]\n",
    "\n",
    "# Generate extra columns\n",
    "distances[\"img1\"] = distances.pair.apply(lambda p: p.split(\" x \")[0])\n",
    "distances[\"img2\"] = distances.pair.apply(lambda p: p.split(\" x \")[1])\n",
    "distances[\"person1\"] = distances.img1.apply(lambda p: p.split(\"_\")[0])\n",
    "distances[\"person2\"] = distances.img2.apply(lambda p: p.split(\"_\")[0])\n",
    "distances[\"same_person\"] = (distances.person1 == distances.person2).apply(lambda s: \"same\" if s else \"different\")\n",
    "\n",
    "# Delete unnecessary columns\n",
    "distances.drop(columns='pair', inplace=True)\n",
    "\n",
    "# Load clusters\n",
    "if CLUSTER_ID is not None:\n",
    "    clusters_ref = pd.DataFrame(data=json.load(open(DLIB_DATASET_CLUSTERS_FILE, \"r\")))\n",
    "    clusters_ref.set_index('label', inplace=True)\n",
    "\n",
    "    distances['img1_cluster'] = distances.img1.apply(lambda i: clusters_ref.cluster.get(i, None))\n",
    "    distances['img2_cluster'] = distances.img2.apply(lambda i: clusters_ref.cluster.get(i, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm (GA) Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET_COLS_TO_IGNORE = [\"resnet_left_ear\", \"resnet_right_ear\", \"resnet_ears\", \"resnet_full_face\"]\n",
    "\n",
    "# Individuals representation\n",
    "resnet_cols = list(filter(lambda c: ('resnet' in c) and (c not in RESNET_COLS_TO_IGNORE), distances.columns))\n",
    "\n",
    "IND_SIZE = len(resnet_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG Search Params\n",
    "SUB_SET_SIZE = 1000000  # Number of distances to consider\n",
    "CXPB = 0.6  # Probability with which two individuals are crossed\n",
    "MUTPB = 0.3 # Probability for mutating an individual\n",
    "INDPB = 0.1  # Probability for flipping a bit of an individual\n",
    "POP_SIZE = 400\n",
    "MAX_GENERATIONS = 100\n",
    "NO_BEST_MAX_GENERATIONS = 10 # Stop search if have a specific number of generations without improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for 1000000 pairs of images of 495 persons\n"
     ]
    }
   ],
   "source": [
    "cleared_distances = distances.replace(inf, np.nan)\n",
    "cleared_distances.dropna(inplace=True)\n",
    "# cleared_distances = cleared_distances[cleared_distances.dlib_distance > 0.01].reset_index(drop=True)\n",
    "cleared_distances = cleared_distances[cleared_distances.img1 != cleared_distances.img2] # Remove same image pairs\n",
    "cleared_distances.sort_values(by='dlib_distance', ascending=True, inplace=True)\n",
    "\n",
    "# Check if we will run the experiment only inside a cluster\n",
    "if CLUSTER_ID is not None:\n",
    "    cleared_distances = cleared_distances[(cleared_distances.img1_cluster == CLUSTER_ID) & (cleared_distances.img2_cluster == CLUSTER_ID)]\n",
    "\n",
    "sub_df = cleared_distances.iloc[:SUB_SET_SIZE]\n",
    "\n",
    "print(f\"Running experiment for {len(sub_df)} pairs of images of {len(sub_df.person1.unique())} persons\")\n",
    "\n",
    "# Normalize distances\n",
    "sub_df = sub_df.loc[:, resnet_cols + [\"dlib_distance\"]] # Get numerical columns to nomrlize\n",
    "for col in sub_df.columns:\n",
    "    sub_df[col] = (sub_df[col]-sub_df[col].min())/(sub_df[col].max()-sub_df[col].min())\n",
    "\n",
    "resnet_distances_norm = sub_df.loc[:, resnet_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Verificar a viabilidade de transformar a fitness para afetar a imagem original com intensidades.\n",
    "\n",
    "# def eval_individual_error(individual):\n",
    "#     \"\"\"\n",
    "#     Calculate the Mean Squeare Error (MSE) of the individual as a measure of fitness\n",
    "#     \"\"\"\n",
    "#     individual_sum = sum(individual)\n",
    "#     individual = [i/individual_sum for i in individual]\n",
    "\n",
    "#     sub_df.loc[:, 'combination'] = resnet_distances_norm.dot(individual)\n",
    "#     sub_df.loc[:, 'error'] = sub_df.combination - sub_df.dlib_distance\n",
    "#     sub_df.loc[:, 'sqr_error'] = (sub_df.error.abs()+1) ** 2 # Avoid squared of fractions\n",
    "\n",
    "#     return (sub_df[sub_df.sqr_error != inf].sqr_error.mean(),) # Shall return a tuple for compatibility with DEAP\n",
    "\n",
    "# def eval_individual_error(individual):\n",
    "#     \"\"\"\n",
    "#     Calculate the Mean Absolute Error (MAE) of the individual as a measure of fitness\n",
    "#     \"\"\"\n",
    "\n",
    "#     individual_sum = sum(individual)\n",
    "#     individual = [i/individual_sum for i in individual]\n",
    "\n",
    "#     sub_df.loc[:, 'combination'] = resnet_distances_norm.dot(individual)\n",
    "#     sub_df.loc[:, 'error'] = sub_df.combination - sub_df.dlib_distance\n",
    "\n",
    "#     return (sub_df[sub_df.error != inf].error.abs().mean(),) # Shall return a tuple for compatibility with DEAP\n",
    "\n",
    "# def eval_individual_error(individual):\n",
    "#     \"\"\"\n",
    "#     Calculate the Absolute Error Sum of the individual as a measure of fitness\n",
    "#     \"\"\"\n",
    "\n",
    "#     individual_sum = sum(individual)\n",
    "#     individual = [i/individual_sum for i in individual]\n",
    "\n",
    "#     sub_df.loc[:, 'combination'] = resnet_distances_norm.dot(individual)\n",
    "#     sub_df.loc[:, 'error'] = sub_df.combination - sub_df.dlib_distance\n",
    "\n",
    "#     return (sub_df[sub_df.error != inf].error.abs().sum(),) # Shall return a tuple for compatibility with DEAP\n",
    "\n",
    "def eval_individual_error(individual):\n",
    "    \"\"\"\n",
    "    Calculate the Step differente of the individual as a measure of fitness\n",
    "    \"\"\"\n",
    "\n",
    "    individual_sum = sum(individual)\n",
    "    if individual_sum == 0:\n",
    "        return (inf,)\n",
    "    individual = [(i/individual_sum) for i in individual]\n",
    "\n",
    "    sub_df.loc[:, 'combination'] = resnet_distances_norm.dot(individual)\n",
    "\n",
    "    # Pandas Like Error\n",
    "    sub_df.loc[:, 'dlib_same_person'] = sub_df.dlib_distance.apply(lambda c: 1 if c < 0.37 else 0)\n",
    "    sub_df.loc[:, 'comb_same_person'] = sub_df.combination.apply(lambda c: 1 if c < 0.37 else 0)\n",
    "    sub_df.loc[:, 'error'] = sub_df.comb_same_person - sub_df.dlib_same_person\n",
    "\n",
    "    return (sub_df[sub_df.error != inf].error.abs().sum(),) # Shall return a tuple for compatibility with DEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,)) # Corr (maximize)\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,)) # Error (minimize)\n",
    "# creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", random)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=IND_SIZE)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", eval_individual_error)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=INDPB)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "pop = toolbox.population(n=POP_SIZE)\n",
    "\n",
    "fitness_time = time()\n",
    "print(f\"Evaluating {POP_SIZE} individuals\")\n",
    "send_simple_message(f\"Evaluating {POP_SIZE} individuals\")\n",
    "# Evaluate the entire population\n",
    "for ind, fit in zip(pop, map(toolbox.evaluate, pop)):\n",
    "    ind.fitness.values = fit\n",
    "print(f\"Time to evaluate fitness {(time() - fitness_time)//60} minutes\")\n",
    "send_simple_message(f\"Opt2 (note): Time to evaluate fitness {(time() - fitness_time)//60} minutes for {POP_SIZE} individuals\")\n",
    "\n",
    "# Extracting all the fitnesses of \n",
    "fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "# Variable keeping track of the number of generations\n",
    "g = 0\n",
    "\n",
    "low_std_times = 0\n",
    "last_max_fit = -1e9\n",
    "last_min_fit = +1e9\n",
    "best_generation = 0\n",
    "bests = []\n",
    "\n",
    "# Begin the evolution\n",
    "while g < MAX_GENERATIONS:\n",
    "    # A new generation\n",
    "    g = g + 1\n",
    "    print(\"-- Generation %i --\" % g)\n",
    "\n",
    "    # Select the next generation individuals\n",
    "    offspring = toolbox.select(pop, len(pop))\n",
    "    \n",
    "    # Clone the selected individuals\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    # Apply crossover and mutation on the offspring\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random() < CXPB:\n",
    "            toolbox.mate(child1, child2)\n",
    "            # Invalidate fitnesses for the new individuals\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "\n",
    "    for mutant in offspring:\n",
    "        if random() < MUTPB:\n",
    "            toolbox.mutate(mutant)\n",
    "            # Invalidate fitnesses for the new individual\n",
    "            del mutant.fitness.values\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness (The new ones)\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    # Replace population with offspring\n",
    "    pop[:] = offspring\n",
    "\n",
    "    # Gather all the fitnesses in one list and print the stats\n",
    "    fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "    length = len(pop)\n",
    "    mean = sum(fits) / length\n",
    "    sum2 = sum(x*x for x in fits)\n",
    "    std = abs(sum2 / length - mean**2)**0.5\n",
    "\n",
    "    print(\"  Min %s\" % min(fits))\n",
    "    print(\"  Max %s\" % max(fits))\n",
    "    print(\"  Avg %s\" % mean)\n",
    "    print(\"  Std %s\" % std)\n",
    "\n",
    "    if std < 3e-3:\n",
    "        low_std_times += 1\n",
    "    \n",
    "    # If we face 5 generations with low standard deviation, let's resset population\n",
    "    if low_std_times > 10:\n",
    "        low_std_times = 0\n",
    "        \n",
    "        # Reset Pop\n",
    "        print(\"Resetting pop\")\n",
    "        pop = toolbox.population(n=POP_SIZE)\n",
    "\n",
    "        # Evaluate the entire population\n",
    "        for ind, fit in zip(pop, map(toolbox.evaluate, pop)):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Extracting all the fitnesses of \n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "    # Minimization (Error)\n",
    "    if min(fits) < last_min_fit:\n",
    "        last_min_fit = min(fits)\n",
    "        best_generation = g\n",
    "        best_idx = fits.index(last_min_fit)\n",
    "        best = pop[best_idx]\n",
    "\n",
    "        print(f\"New best found (Gen: {best_generation}): {last_min_fit}\")\n",
    "        send_simple_message(f\"New best found (Gen: {best_generation}): {last_min_fit}\")\n",
    "        json.dump(dict(zip(resnet_cols, best)), open(BEST_INDIVIDUAL_FILE, 'w'))\n",
    "\n",
    "        bests.append(\n",
    "                        {\n",
    "                            \"generation\": best_generation,\n",
    "                            \"fitness\": last_min_fit,\n",
    "                            \"best_data\": dict(zip(resnet_cols, best))\n",
    "                        }\n",
    "                    )\n",
    "        json.dump(bests, open(BEST_INDIVIDUALS_FILE, 'w'))\n",
    "    \n",
    "    if g - best_generation > NO_BEST_MAX_GENERATIONS:\n",
    "        print(\"No best found for too long, ending search\")\n",
    "        send_simple_message(\"No best found for too long, ending search\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Done: {g} generations. Best fitness: {last_min_fit} at generation {best_generation} in {(time() - start_time)//60} minutes\")\n",
    "_ = send_simple_message(f\"Done: {g} generations. Best fitness: {last_min_fit} at generation {best_generation} in {(time() - start_time)//60} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf2_resnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "599a4db38103377133a9d21b7d02324d7e7685cce2f1b75d4c68b493ddfcf076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
